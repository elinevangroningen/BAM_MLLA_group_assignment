---
title: "K-Nearest Neighbors Algorithm"
output: html_notebook
---

## Set up turning grid

```{r}
var_levels <- sapply(data_train, levels)
length(var_levels)
vars_multiple_levels <- c()
for(i in 1:length(var_levels)) {
  if(length(var_levels[names(var_levels[i])][[1]]) > 2) {
    vars_multiple_levels <- append(vars_multiple_levels, names(var_levels[i]))
  }
}
vars_multiple_levels
```


```{r}
# Generate tuning grid for knn
knn_tune_grid <- tibble(neighbors = 1:50*2-1)
knn_tune_grid
```

## Specify a workflow 

Something that should be noted for this recipe, is that only numeric variables are included. This is done for the reason that categorical variables translate with difficulty to a k-nearest neighbor algorithm. The premise of prediction based on a KNN-model is that it relies exclusively on the distance between points in the data. This distance is obvious when handling numeric variables. However, when dealing with non-numeric values and variables this distance between data points cannot easily be modeled, provided they should be modeled at all. (This will have implications for determining predictions for importance and coefficients for variables, which will be addressed at the end of the section on the KNN-model.)
```{r}
# Specify model 
knn_mod <- 
  nearest_neighbor(neighbors = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("kknn", scale=FALSE)
knn_mod

# Specify recipe
knn_recipe <- 
  recipe(price ~ ., data = data_train) %>% 
  step_rm(property_type, room_type, host_response_time, neighbourhood_cleansed,
          neighbourhood_cleansed) %>%
  step_dummy(all_nominal()) %>%
  update_role(id, new_role = "id var") %>% 
  step_normalize(all_predictors(), -id)
knn_recipe
```

The normalization of the data is ensured through the following commands:
```{r}
# Check normalization 
train_baked <- knn_recipe %>% prep(data_train) %>% bake(data_train)
train_baked %>% head()
round(colMeans(train_baked, 8))
round(apply(train_baked, 2, sd), 8)
rm(train_baked)
```

Below is the initial workflow for the k-nearest neighbors model is specified
```{r}
# Specify workflow
knn_workflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(knn_recipe)
knn_workflow
```

## Tuning the number of nearest neighbours

The code below serves to specify the assessment metrics that are used. Moreover, a grid search is performed using the validation sets. 
```{r}
# Store metrics in variable
metrics_reg <- metric_set(rmse, mae, rsq_trad)

# Perform grid search using validation sets
knn_tune_res <- 
  knn_workflow %>% 
  tune_grid(resamples = data_folds,
            grid = knn_tune_grid, 
            metrics = metrics_reg) 

# Plot results metrics
knn_metrics_plot <- 
  knn_tune_res %>%  collect_metrics() %>% 
  ggplot(aes(x = neighbors, y = mean)) +
  geom_point() + geom_line() +
  facet_wrap(~ .metric, scale = "free_y") +
  labs(title = "KNN Performance Metrics") +
  theme(plot.title = element_text(hjust = 0.5)) 
knn_metrics_plot

# Save peformance plot
ggsave("plots/knn_neighbor_metrics.png", plot = knn_metrics_plot)

autoplot(knn_tune_res)
```
The plot output shows some metrics that plot the mean of the performance metrics. We should aim for the mae (mean absolute error) and rmse (root mean square error) to be as low as possible, and the rsq_trad (R-squared) to be as high as possible. We used the mae metric to determine the optimal k-neighbors for our model, which arrived at 51 neighbors. This can be read from the mae graph, by looking at the corresponsing k-neighbors for the lowest mean of mae. 

Moreover, from the last plot the elbow trend can somewhat clearly be seen: the metrics reach their optimum point after which the level off and slowly increase for mae and rmse and decrease for rqs_trad.

The model with the optimal number of k-nearest neighbors can then be selected as follows:
```{r}
# Generate best model
knn_best_model <- select_best(knn_tune_res, metric = "mae")
```

## Finalize workflow

Below the finalized workflow is made, which automatically picks the best KNN-model defined above (which is specified by the mae metric)
```{r}
# Finalize workflow
knn_workflow_final <- 
  knn_workflow %>% 
  finalize_workflow(knn_best_model)
knn_workflow_final
```

## Last fit 

A final workflow can be set up to check the final fit. Furthermore, the performance metrics for the best KNN-model are selected and put in a table.
```{r}
# Train and test the data set
knn_last_fit <- 
  knn_workflow_final %>% 
  last_fit(data_split, 
           metrics = metrics_reg)

# Collect KNN performance metrics
knn_metrics <- 
  knn_last_fit %>% 
  collect_metrics %>% 
  select(-.estimator) %>% 
  mutate(model = "knn reg")
knn_metrics
```

# KNN variable importance 
KNN, as a method, does not come with a prediction for the importance or coefficients of variables. The reason for this has to do with the fact that prediction in a k-nearest neighbor model relies exclusively on the distance between data points. With this comes the added implication that no information about the relative importance of variables can be derived from it.

# Metrics

Below the final workflow with the optimal k-neighbors is selected and ran against the test set.
```{r}
# Generate predicted values for price
knn_test_preds <- 
  knn_workflow_final %>% 
  fit(data = data_train) %>%
  predict(data_test) %>% 
  pull(.pred)

# Create tibble for distribution plot
knn_pred <- 
  tibble(observed = data_test$price, 
         predicted = knn_test_preds, 
         residual = observed - predicted)

# Plot distribution residuals
knn_residual_plot <- 
  knn_pred %>% 
  ggplot(aes(x = residual)) +
  geom_density(bw = 0.15, fill = "springgreen", alpha = 0.5) +
  geom_rug() +
  labs(title = "KNN Distribution Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_cartesian(ylim = c(0, 1.5))
knn_residual_plot

# Save plot for presentation
ggsave("plots/knn_residual.png", plot = knn_residual_plot)
```
Finally, a distribution plot is then set up to check how well the KNN-model fit for the actual observed prices. As you can see the predictions are fairly normally distributed, meaning there is no skewedness to either one side. This means that there are no extra implications that should be kept in mind when interpreting the mean absolute error (mae).















---
title: "RF"
author: "Kylian van Noordenne"
date: "22-11-2020"
output: html_document
---

Within this section, a random forest will be created. FIrst a preprocessing recipe is created. 

```{r libraries }
library(ranger)
library(doParallel)
library(themis)
library(tibble)
library(vip)
```

Setting up a recipe. The id variable is uptaded to a seperate role, instead of being a predictor. 
```{r Recipe}
rf_recipe <- recipe(price ~ ., data = data_train) %>%
  update_role(id, new_role = "id var")

rf_recipe
```


```{r Specify}
# Tune specification
tune_spec <- rand_forest(mtry = tune(), trees = 200) %>%
  set_engine("ranger")
```

Combine the recipe and the model into a workflow that can be tuned.
```{r Workflow}
# Workflow creation
tune_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(tune_spec)
```

We create a metric set that calculates the Root Mean Square Error (rmse), the Mean Absolute Error (mae) and the R-squared (rsq_trad)
```{r Class metrics}
class_metrics <- metric_set(rmse, mae, rsq_trad)
```

The command bellow allows us to do computations in parallel
```{r Parallel}
registerDoParallel()
```

Here I tune the grid. The values chosen as mtry are 6, 10, 15, 16 & 21, as these were my best values, before tuning my grid (based on the lower MAE criteria).
```{r Tune grid}
tunegrid <- tibble(mtry = c(6, 10, 15, 16, 21))
```

```{r Grid}
# Tune the grid
set.seed(12345)
tune_res <- tune_grid(
  tune_wf,
  resamples = data_folds,
  grid = tunegrid,
  metrics = class_metrics
)
tune_res
```

```{r Collect metrics}
tune_res %>%
  collect_metrics()
```
A plot for finding the best mtry, based on the criteria of the mae. A lower mae would indicate a better results, as a lower value indicates a lower error of prediction. 
```{r Plot MAE}
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "mae") %>% 
  ggplot(aes(x = mtry, y = mean)) +
  geom_line() +
  geom_point() +
  labs(y = "mae")
```

This command allows me to see the best predictions based on the MAE criteria. 
```{r Best MAE}
tune_res %>% show_best("mae")
```
Now select the best model based on the mae criteria
```{r Best model}
best_rmse <- select_best(tune_res, "mae")
final_rf <- finalize_workflow(tune_wf, best_rmse)
final_rf
```

Now we can train the finalized workflow on our entire training rest
```{r }
# Finalise workflow on training set
final_res <- final_rf %>%
  last_fit(data_split, metrics = class_metrics)
```

The results based on the test set will be 
```{r }
# Score on test data
final_res %>%
  collect_metrics()
```

Now we try to asses the variable importance. We will refit the model based on our previous tune parameters. 
```{r }
# Refit the model
rf_model_vi <- rand_forest(mtry = 6, trees = 200) %>%
  set_engine("ranger", importance = "permutation")

rf_vi_wf <- workflow() %>% 
  add_model(rf_model_vi) %>% 
  add_recipe(rf_recipe)

# Fit the model again
set.seed(12345)
rf_vi_fit <- rf_vi_wf %>% fit(data = data_train)
```

```{r }
rf_vi_fit %>% pull_workflow_fit() %>% vi()

rf_vi_fit %>% pull_workflow_fit() %>% vip(geom = "point")
```
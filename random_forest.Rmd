---
title: "RF"
author: "Kylian van Noordenne"
date: "22-11-2020"
output: html_document
---

# Packages 
```{r libraries }
library(ranger)
library(doParallel)
library(themis)
library(tibble)
library(vip)
```

# Recipe
Within this section, a random forest will be created. First a preprocessing recipe is created. 
The id variable is updated to a seperate role, instead of being a predictor. 
```{r Recipe}
rf_recipe <- recipe(price ~ ., data = data_train) %>%
  update_role(id, new_role = "id var")

rf_recipe
```

# Specify the random forest
Within this section the tune specificaiton are mentioned. The *mtry* is the number of features that are used at each split. THe exact mtry value will be tuned later on. Different values for trees where tested (200, 500 & 1000). Increasing the amount of trees did not have much impact on the results. Therefore, a tree size of 200 is chosen to save computational time. 
```{r message=FALSE}
# Tune specification
tune_spec <- rand_forest(mtry = tune(), trees = 200) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

# Workflow creation for tuning
Combine the recipe and the model into a workflow that can be tuned.
```{r message=FALSE}
# Workflow creation
tune_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(tune_spec)
```

A metric set that calculates the *Root Mean Square Error (rmse)*, *the Mean Absolute Error (mae)* and the *R-squared (rsq_trad)* is created. 
```{r Class metrics}
# Class metrics specification 
class_metrics <- metric_set(rmse, mae, rsq_trad)
```

The command bellow allows us to do computations in parallel.
```{r message=FALSE}
registerDoParallel()
```

The tune grid was initially not optimized, but the command grid = tibble(mtry = 1:33) was utilized. This command checked all the variables. Based on the MAE criteria, a mtry of 5, 6, 7, 8 & 9 was found as the optimal solution. After this finding, these optimal mtry values were taken as mtry input to save computational time.  
```{r message=FALSE}
# Define the tune grid 
tunegrid <- tibble(mtry = c(1:10))
```

```{r Grid}
# Tune the grid
set.seed(12345)
tune_res <- tune_grid(
  tune_wf,
  resamples = data_folds,
  grid = tunegrid,
  metrics = class_metrics
)
tune_res
```

# Selecting tuning parameters 
```{r Collect metrics}
# Collect metrics
tune_res %>%
  collect_metrics()
```
A plot for finding the best mtry, based on the criteria of the mae. A lower mae would indicate a better results, as a lower value indicates a lower error of prediction. 
```{r Plot MAE}
# Plot results all metrics
tune_res %>%
  collect_metrics() %>%
  filter(.metric %in% c("rmse", "mae", "rsq_trad")) %>%
  ggplot(aes(x = mtry, y = mean, ymin = mean - std_err, ymax = mean + std_err, 
             colour = .metric)) +
  geom_errorbar() + 
  geom_line() +
  geom_point() +
  facet_grid(.metric ~ ., scales = "free_y") 

# Plot the MAE based
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "mae") %>% 
  ggplot(aes(x = mtry, y = mean, ymin = mean - std_err, ymax = mean + std_err)) +
  geom_errorbar() + 
  geom_line() +
  geom_point() +
  labs(y = "mae")
```

This command allows will show the best mtry based on the MAE criteria. However, initially it showed 6, 9, 14, 15 and 20. THerefore, these values are shown again as the grid is limited to only using those optimal mtry values.  
```{r Best MAE}
# Find the mtry with the best MAE
tune_res %>% show_best("mae")
```

# Best model selection

The best model based on the MAE criteria is selected and eventually finalises into the workflow. 
```{r Best model}
# Best model selection
best_rmse <- select_best(tune_res, "mae")
final_rf <- finalize_workflow(tune_wf, best_rmse)
final_rf
```

# Test set performance

Now we can train the finalized workflow on our entire training rest
```{r Finalise}
# Finalise workflow on training set
final_res <- final_rf %>%
  last_fit(data_split, metrics = class_metrics)
```

The results based on the test set will be 
```{r Test results}
# Score on test data
set.seed(54321)
final_res %>%
  collect_metrics()
```

# Variable importance 

Now we try to asses the variable importance. We will refit the model based on our previous tune parameters. 
```{r Refit}
# Refit the model
rf_model_vi <- rand_forest(mtry = 6, trees = 200) %>%
  set_engine("ranger", importance = "permutation")

rf_vi_wf <- workflow() %>% 
  add_model(rf_model_vi) %>% 
  add_recipe(rf_recipe)

# Fit the model again
set.seed(12345)
rf_vi_fit <- rf_vi_wf %>% fit(data = data_train)
```

We can use the refitted model in order the gather the variable importance 
```{r Variable importance}
# Variable importance 
rf_vi_fit %>% pull_workflow_fit() %>% vi()
```

The variable importance indicates that the accommodates, bedrooms and room_type are the most important variables for predicting the logprice. The variables which are the least important for predicting the logprice,are bed_type, pool, and wifi. Pool and wifi actually have a negative importance, bus as this is close to a value of 0, it is chosen to still include those variables.

```{r}
# Plot variable importance
var_importance_plot <-
  rf_vi_fit %>%
  pull_workflow_fit() %>% vip(geom = "point", num_features = 12) +
  labs(title = "Random Forest Variable Importance") +
  theme(plot.title = element_text(hjust = 0.5)) 
rf_vi_fit

# Save plot for presentation
ggsave("plots/rf_var_importance.png", plot = var_importance_plot,
       height = 7 , width = 10)

```

```{r}
# Generate predicted values for sales
set.seed(12345)
rf_test_preds <- 
  rf_vi_wf %>% 
  fit(data = data_train) %>%
  predict(data_test) %>% 
  pull(.pred)

# Create tibble for distribution plot
rf_pred <- 
  tibble(observed = data_test$price, 
         predicted = rf_test_preds, 
         residual = observed - predicted)

# Plot distribution residuals
rf_residual_plot <- 
  rf_pred %>% 
  ggplot(aes(x = residual)) +
  geom_density(bw = 0.15, fill = "springgreen", alpha = 0.5) +
  geom_rug() +
  labs(title = "Random Forest Distribution Residuals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_cartesian(ylim = c(0, 1.5))
rf_residual_plot

# Save plot for presentation
ggsave("plots/rf_residual.png", plot = rf_residual_plot)
```


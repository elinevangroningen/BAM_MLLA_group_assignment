---
title: "RF"
author: "Kylian van Noordenne"
date: "22-11-2020"
output: html_document
---

Within this section, a random forest will be created. FIrst a preprocessing recipe is created. 

```{r libraries }
library(ranger)
library(doParallel)
library(themis)
library(tibble)
library(vip)
```


```{r }
data_final$logprice <- log(data_final$price)

data_final$price <- NULL

# Summary(data_final$logprice) #has inf values because of log(0)

data_final$logprice[which(!is.finite(data_final$logprice))] <- 0 #add 0 where the value is inf

# Repeat train split as the data in final data was modified
# Create a train-split sets 

set.seed(123)
data_split <- initial_split(data_final, prop = 0.7)
data_train <- training(data_split)
data_test <- testing(data_split)

# Generate 10-fold CV sets
set.seed(321)
data_folds <- vfold_cv(data_train, v = 10)
data_folds
``` 

Setting up a recipe. The id variable is uptaded to a seperate role, instead of being a predictor. 
```{r Recipe}


rf_recipe <- recipe(logprice ~ ., data = data_train) %>%
  update_role(id, new_role = "id var")

rf_recipe
```

Within this section the tune specificaiton are mentioned. THe mtry will be tuned later on. Different values for trees where tested (200, 500 & 1000). As is indicated that increase the amount of trees did not have much impact on the result, trees of 200 are chosen to save computational time. 
```{r Specify}
# Tune specification
tune_spec <- rand_forest(mtry = tune(), trees = 200) %>%
  set_engine("ranger")
```

Combine the recipe and the model into a workflow that can be tuned.
```{r Workflow}
# Workflow creation
tune_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(tune_spec)
```

We create a metric set that calculates the Root Mean Square Error (rmse), the Mean Absolute Error (mae) and the R-squared (rsq_trad)
```{r Class metrics}
# Class metrics specification 
class_metrics <- metric_set(rmse, mae, rsq_trad)
```

The command bellow allows us to do computations in parallel
```{r Parallel}
registerDoParallel()
```

Here I tune the grid. I first did not specify my grid and found that the optimal solutions, based on the MAE criteria, were a mtry of 6, 10, 15, 16 & 21. Therefore, these values were taken as mtry input to save computational time.  
```{r Tune grid}
# Define the tune grid 
tunegrid <- tibble(mtry = c(6, 10, 15, 16, 21))
```

```{r Grid}
# Tune the grid
set.seed(12345)
tune_res <- tune_grid(
  tune_wf,
  resamples = data_folds,
  grid = 10,
  metrics = class_metrics
)
tune_res
```

```{r Collect metrics}
# Collect metrics
tune_res %>%
  collect_metrics()
```
A plot for finding the best mtry, based on the criteria of the mae. A lower mae would indicate a better results, as a lower value indicates a lower error of prediction. 
```{r Plot MAE}
# Plot the MAE based
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "mae") %>% 
  ggplot(aes(x = mtry, y = mean, ymin = mean - std_err, ymax = mean + std_err)) +
  geom_errorbar() + 
  geom_line() +
  geom_point() +
  labs(y = "mae")
```

This command allows me to see the best predictions based on the MAE criteria. 
```{r Best MAE}
# Find the mtry with the best MAE
tune_res %>% show_best("mae")
```
Now select the best model based on the MAE criteria and finalise the workflow 
```{r Best model}
# Best model selection
best_rmse <- select_best(tune_res, "mae")
final_rf <- finalize_workflow(tune_wf, best_rmse)
final_rf
```

Now we can train the finalized workflow on our entire training rest
```{r }
# Finalise workflow on training set
final_res <- final_rf %>%
  last_fit(data_split, metrics = class_metrics)
```

The results based on the test set will be 
```{r }
# Score on test data
set.seed(54321)
final_res %>%
  collect_metrics()
```

Now we try to asses the variable importance. We will refit the model based on our previous tune parameters. 
```{r }
# Refit the model
rf_model_vi <- rand_forest(mtry = 6, trees = 200) %>%
  set_engine("ranger", importance = "permutation")

rf_vi_wf <- workflow() %>% 
  add_model(rf_model_vi) %>% 
  add_recipe(rf_recipe)

# Fit the model again
set.seed(12345)
rf_vi_fit <- rf_vi_wf %>% fit(data = data_train)
```

We can use the refitted model in order the gather the variable importance 
```{r }
# Variable importance 
rf_vi_fit %>% pull_workflow_fit() %>% vi()
```
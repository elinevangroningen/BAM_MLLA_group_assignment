---
title: "k-nearest_neigbor"
output: html_document
---

##Packages used

```{r}
library("tidyverse")
library("tidymodels")
library("knitr")
```


##K-nearest neighbor Regression

Below is specified how the hyperparameters are set for the validation split that is necessary for a knn-model.
```{r}
set.seed(91231)
data_train_val <- validation_split(data_train, prop = 0.7)

data_train_val
```


###Setting up a tuning grid

Here it is specified for how many neighbors we want to run our model. The chosen k-neighbors that are analyzed for this model are 1 to 15 neighbors.
```{r}
knn_regr_tune_grid <- tibble(neighbors = 1:30*2 - 1)
knn_regr_tune_grid
```


###Specifying corresponding workflow

Here the specification of the knn-model is given. The mode is set to 'regression', instead of 'classification', because the price variable we will be working with is numeric and not logistic variable. Moreover, the computational engine is set with set_engine().
```{r}
knn_regr_mod <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("kknn", scale = FALSE)
```

Through the following command you can see that this model is used for training the model
```{r}
knn_regr_mod %>% translate()
```

###Specifying the recipe for the knn-model
Creating the recipe, and ensuring normalization of all predictors.

Something that should be noted for this recipe, is that only numeric variables are included. This is done for the reason that categorical variables translate with difficulty to a k-nearest neighbor algorithm. The premise of prediction based on a knn-model is that it relies exclusively on the distance between points in the data. This distance is obvious when handling numeric variables. However, when dealing with non-numeric values and variables this distance between data points cannot easily be modeled, provided they should be modeled at all. (This will have implications for determining predictions for importance and coefficients for variables, which will be addressed at the end of the section on the knn-model.)
```{r}
knn_regr_recipe <- 
  recipe(price ~ ., data = data_train) %>% 
  step_rm(id, property_type, room_type, bed_type, host_since, host_response_time, host_identity_verified, neighbourhood_cleansed, instant_bookable, cancellation_policy, require_guest_profile_picture, require_guest_phone_verification, wifi, pool, hot_tub, tv, host_email, host_phone, host_facebook, host_government_id) %>%
  step_normalize(all_predictors())
```

Overview of the recipe
```{r}
knn_regr_recipe
```


```{r}
data_train_baked <- knn_regr_recipe %>% prep(data_train) %>% bake(data_train)
data_train_baked %>% head()
```

The workflow then is:
```{r}
knn_regr_workflow <-
  workflow() %>% 
  add_model(knn_regr_mod) %>% 
  add_recipe(knn_regr_recipe)

knn_regr_workflow
```

The 'data_train_baked' can be removed because it is not necessary anymore
```{r}
rm(data_train_baked)
```


##Tuning the number of nearest neighbors

A grid search is used to search over the grid for potential values, by using a cross-validation set as follows:
```{r}
knn_regr_tune_res <- knn_regr_workflow %>% 
  tune_grid(resamples = data_folds, 
            grid = knn_regr_tune_grid,
            metrics = metric_set(rmse, rsq_trad, mae))
```

The metrics specified in the previous command can be collected as follows:
```{r}
knn_regr_tune_res %>% collect_metrics()
```

These metrics can be plotted as well
```{r}
knn_regr_tune_res %>% collect_metrics() %>% 
  ggplot(aes(x = neighbors, y = mean)) + 
  geom_point() + geom_line() + 
  facet_wrap(~ .metric, scales = "free_y")
```

Using the validation set, we can select the best k neighbors, by looking at the different metrics
```{r}
knn_regr_tune_res %>% 
  show_best("mae", n = 3) %>% 
  arrange(neighbors)
```


##Finalizing the workflow

Before the workflow can be finalized, the information which shows which model has the best value for the tuning parameter
```{r}
knn_regr_best_model <- select_best(knn_regr_tune_res, metric = "mae")
knn_regr_best_model
```

A finalized workflow, which specifies which k neighbors parameter that is used from now on (the best performing number of k-neighbors)
```{r}
knn_regr_workflow_final <- 
  knn_regr_workflow %>% 
  finalize_workflow(knn_regr_best_model)
```

This can be retained on the entire training set as follows:
```{r}
knn_regr_workflow_final %>% fit(data = data_train)
```


##Prediction of importance and coefficients for variables: An additional note on the k-nearest neighbors algorithm
KNN, as a method, does not come with a prediction for the importance or coefficients of variables. The reason for this has to do with the fact that prediction in a k-nearest neighbor model relies exclusively on the distance between data points. With this comes the added implication that no information about the relative importance of variables can be derived from it.